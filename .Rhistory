reps = 10
K = 5
results = data.frame(Replication = integer(),Fold = integer(),
Threshold = numeric(),f1 = numeric(),f1_train = numeric())
for (rep in 1:reps) {
set.seed(10000*rep)
folds = rep(1:K, ceiling(nrow(data)/K))
folds = sample(folds)
folds = folds[1:nrow(data)]
for(k in 1:K){
k_train = which(folds != k)
k_validation = setdiff(1:nrow(data), k_train)
train_data = data[k_train,]
validation_data = data[k_validation,]
x_scale = scale(train_data[, -1])
train_data[, -1] = x_scale
validation_data[, -1] = scale(validation_data[, -1],
center = attr(x_scale, "scaled:center"),
scale = attr(x_scale, "scaled:scale"))
log_reg_fit = glm(y ~ ., data = train_data, family = "binomial")
probs = predict(log_reg_fit, newdata = validation_data, type = "response")
best_threshold = NULL
best_f1 = -Inf
y_test = as.factor(ifelse(validation_data$y == "break", 1, 0))
for (threshold in seq(0, 1, by = 0.05)) {
predictions = ifelse(probs >= threshold, 1, 0)
f1 = f1_score(as.factor(predictions), y_test)
if (f1 > best_f1) {
best_f1 = f1
best_threshold = threshold
}
}
results = rbind(results, data.frame(Replication = rep,Fold = k,
Threshold = best_threshold,f1 = best_f1))
}
}
#Importing libraries
library(ROCR)
library(kernlab)
#loading the dataset
load("data_assignment_processminer.Rdata")
#Seperating Train and test datas
n = nrow(data_processminer)
set.seed(10000)
test_index = sample(1:n, n * 0.2)
test_data = data_processminer[test_index,]
index = setdiff(1:n, test_index)
data = data_processminer[index,]
#calculating number of 'break' and 'no_break' class
no_of_break = data[data$y == "break",]
no_of_no_break = data[data$y == "no_break",]
cat("No of break class in the data set:", nrow(no_of_break),
"\nNo of no_break class in the data set:", nrow(no_of_no_break))
#Creating a function to calculate f1 score
f1_score = function(predictions, y){
confusion = table(predictions, y)
if (all(dim(confusion) == c(2, 2))) {
precision = ifelse(sum(confusion[2, ]) == 0, 0,
confusion[2, 2] / sum(confusion[2, ]))
recall = ifelse(sum(confusion[, 2]) == 0, 0,
confusion[2, 2] / sum(confusion[, 2]))
if ((precision + recall) == 0) {f1 = 0}
else {f1 = (2 * precision * recall) / (precision + recall)}
}else {f1 = 0}
return(f1)
}
#Calculating other matrics
calculate_metrics <- function(predictions, actual) {
confusion = table(predictions, actual)
if (all(dim(confusion) == c(2, 2))) {
TP = confusion[2, 2]
TN = confusion[1, 1]
FP = confusion[1, 2]
FN = confusion[2, 1]
sensitivity = TP / (TP + FN)
specificity = TN / (TN + FP)
f1 = f1_score(predictions, actual)
return(c(sensitivity = sensitivity, specificity = specificity, f1 = f1))
} else {
return(c(NA, NA, NA))
}
}
reps = 10
K = 5
results = data.frame(Replication = integer(),Fold = integer(),
Threshold = numeric(),f1 = numeric(),f1_train = numeric())
for (rep in 1:reps) {
set.seed(10000*rep)
folds = rep(1:K, ceiling(nrow(data)/K))
folds = sample(folds)
folds = folds[1:nrow(data)]
for(k in 1:K){
k_train = which(folds != k)
k_validation = setdiff(1:nrow(data), k_train)
train_data = data[k_train,]
validation_data = data[k_validation,]
x_scale = scale(train_data[, -1])
train_data[, -1] = x_scale
validation_data[, -1] = scale(validation_data[, -1],
center = attr(x_scale, "scaled:center"),
scale = attr(x_scale, "scaled:scale"))
log_reg_fit = glm(y ~ ., data = train_data, family = "binomial")
probs = predict(log_reg_fit, newdata = validation_data, type = "response")
best_threshold = NULL
best_f1 = -Inf
y_test = as.factor(ifelse(validation_data$y == "break", 1, 0))
for (threshold in seq(0, 1, by = 0.05)) {
predictions = ifelse(probs >= threshold, 1, 0)
f1 = f1_score(as.factor(predictions), y_test)
if (f1 > best_f1) {
best_f1 = f1
best_threshold = threshold
}
}
results = rbind(results, data.frame(Replication = rep,Fold = k,
Threshold = best_threshold,f1 = best_f1))
}
}
best_threshold = mean(results$Threshold)
cat("Average of best threshold value for Logistic Regression:", best_threshold)
validation_data
validation_data[, -1]
